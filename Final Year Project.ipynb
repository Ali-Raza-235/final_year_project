{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ead2880",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc1a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4b6f5",
   "metadata": {},
   "source": [
    "# Task 1: Preproccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55bc0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    words = doc.lower().translate(translator).split()\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def preprocess_data(docList):\n",
    "    return [preprocessing(doc) for doc in docList]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733aae81",
   "metadata": {},
   "source": [
    "# Task 2: Count Lines & Count Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21f973de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return len(file.readlines())\n",
    "\n",
    "def count_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return len(file.read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b214b1",
   "metadata": {},
   "source": [
    "# Task 3: Generate DataFrame with stop words and their count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "531bf2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_count_df(docList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words_count = {word: sum(doc.lower().count(word) for doc in docList) for word in stop_words}\n",
    "    stop_words_df = pd.DataFrame({'Stop Word': list(stop_words_count.keys()), 'Count': list(stop_words_count.values())})\n",
    "    return stop_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa71fef",
   "metadata": {},
   "source": [
    "# Task 4: Generate .txt file without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "238dbce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(docList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for idx, doc in enumerate(docList):\n",
    "        filename = f\"BC190406208_exclude_stopwords_{os.path.splitext(os.path.basename(doc_names[idx]))[0]}.txt\"\n",
    "        cleaned_content = \" \".join(word for word in preprocessing(doc) if word not in stop_words)\n",
    "        output_file_path = os.path.join(directory_path, filename)  # Path for the new file\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(cleaned_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd4708b",
   "metadata": {},
   "source": [
    "# Task 5: Count of Lowercase Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1500443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_words_count(docList):\n",
    "    lowercase_count = sum(sum(1 for word in preprocessing(doc) if word.islower()) for doc in docList)\n",
    "    print(f\"Count of lowercase words: {lowercase_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e9418",
   "metadata": {},
   "source": [
    "# Task 6: Perform Lemmatization & generate .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "914f8aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lemmatization(docList):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for idx, doc in enumerate(docList):\n",
    "        filename = f\"BC190406208_lemmatized_{os.path.splitext(os.path.basename(doc_names[idx]))[0]}.txt\"\n",
    "        words = word_tokenize(doc)\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        lemmatized_content = ' '.join(lemmatized_words)\n",
    "        output_file_path = os.path.join(directory_path, filename)  # Path for the new file\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(lemmatized_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302ace3",
   "metadata": {},
   "source": [
    "#  Task 7: Perform Stemming & generate .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4792be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_stemming(docList):\n",
    "    stemmer = PorterStemmer()\n",
    "    for idx, doc in enumerate(docList):\n",
    "        filename = f\"BC190406208_stemming_{os.path.splitext(os.path.basename(doc_names[idx]))[0]}.txt\"\n",
    "        words = word_tokenize(doc)\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        stemmed_content = ' '.join(stemmed_words)\n",
    "        output_file_path = os.path.join(directory_path, filename)  # Path for the new file\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(stemmed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4f9bb",
   "metadata": {},
   "source": [
    "# Task 8: Read All .txt files & Creating List of Words for every document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc015fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_files_from_directory(directory_path):\n",
    "    docList = []\n",
    "    doc_names = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt') and os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                docList.append(file.read())\n",
    "                doc_names.append(filename)\n",
    "    return docList, doc_names\n",
    "\n",
    "def wordList(doc):\n",
    "    return words_list(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39c169",
   "metadata": {},
   "source": [
    "# Task 9: Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d479f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePuncs(wordList):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_words = [word for word in wordList if word not in stop_words]\n",
    "    # Remove punctuation from each word\n",
    "    cleaned_words = [''.join(char for char in word if char not in string.punctuation) for word in cleaned_words]\n",
    "    cleaned_words = [word for word in cleaned_words if word.isalpha()]\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed36e9",
   "metadata": {},
   "source": [
    "# Task 10: Term Frequency in Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "843948cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequencyInDoc(wordList):\n",
    "    total_words = len(wordList)\n",
    "    term_freq_dict = {}\n",
    "    for word in wordList:\n",
    "        term_freq_dict[word] = term_freq_dict.get(word, 0) + 1\n",
    "    term_frequency_dict = {word: freq / total_words for word, freq in term_freq_dict.items()}\n",
    "    return term_frequency_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9757cdd",
   "metadata": {},
   "source": [
    "#  Task 11: Word Doc Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87117842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordDocFrequency(dicList):\n",
    "    word_doc_freq_dict = {}\n",
    "    for doc_dict in dicList:\n",
    "        words_in_doc = set(doc_dict.keys())\n",
    "        for word in words_in_doc:\n",
    "            word_doc_freq_dict[word] = word_doc_freq_dict.get(word, 0) + 1\n",
    "    return word_doc_freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70db250",
   "metadata": {},
   "source": [
    "# Task 12: Inverse Doc Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "694073ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocFrequency(dicList):\n",
    "    total_docs = len(dicList)\n",
    "    word_idf_dict = {}\n",
    "    for word, doc_freq in dicList.items():\n",
    "        idf = 1 + math.log(total_docs / (1 + doc_freq))\n",
    "        word_idf_dict[word] = idf\n",
    "    return word_idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f994a2b",
   "metadata": {},
   "source": [
    "# Task 13: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d319891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(docList):\n",
    "    tfidf_list = []\n",
    "    dicList = [termFrequencyInDoc(removePuncs(preprocessing(doc))) for doc in docList]\n",
    "    vocabulary = wordDocFrequency(dicList)\n",
    "    idf = inverseDocFrequency(vocabulary)\n",
    "    for doc in docList:\n",
    "        words = removePuncs(preprocessing(doc))\n",
    "        term_freq_dict = termFrequencyInDoc(words)\n",
    "        tfidf_dict = {word: tf * idf[word] for word, tf in term_freq_dict.items()}\n",
    "        tfidf_list.append(tfidf_dict)\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3aab4",
   "metadata": {},
   "source": [
    "# Task 14: Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fbda154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorSpaceModel(query, tfidf_list, docList):\n",
    "    query_words = removePuncs(preprocessing(query))\n",
    "    query_term_freq = termFrequencyInDoc(query_words)\n",
    "\n",
    "    query_idf = {}\n",
    "    for word in query_term_freq.keys():\n",
    "        if word in idf:\n",
    "            query_idf[word] = idf[word]\n",
    "        else:\n",
    "            query_idf[word] = 1\n",
    "\n",
    "    query_vector = {word: tf * query_idf[word] for word, tf in query_term_freq.items()}\n",
    "\n",
    "    similarity_scores = defaultdict(float)\n",
    "    for doc_idx, doc_vector in enumerate(tfidf_list):\n",
    "        for word, tfidf in doc_vector.items():\n",
    "            if word in query_vector:\n",
    "                similarity_scores[doc_idx] += query_vector[word] * tfidf\n",
    "\n",
    "    sorted_documents = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbddfdb",
   "metadata": {},
   "source": [
    "Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69e254e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Computer Graphics\",\n",
    "    \"natural settings\",\n",
    "    \"natural language system\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679b2e6",
   "metadata": {},
   "source": [
    "Reading all text files from the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "100250cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"ACL_DATASET\"\n",
    "directory_path = os.path.join(os.getcwd(), folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e794c35",
   "metadata": {},
   "source": [
    "Calling All Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6840f23a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of lowercase words: 179856\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read all text files from the ACL_DATASET folder\n",
    "    docList, doc_names = read_text_files_from_directory(directory_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The folder '{folder_name}' was not found in the current directory.\")\n",
    "    exit()\n",
    "\n",
    "# Task 1: Preprocessing of data\n",
    "preprocessed_docList = preprocess_data(docList)\n",
    "\n",
    "# Task 2: Count number of lines and words in each text file\n",
    "line_counts = [count_lines(os.path.join(directory_path, file)) for file in doc_names]\n",
    "word_counts = [count_words(os.path.join(directory_path, file)) for file in doc_names]\n",
    "\n",
    "# Task 3: Generate DataFrame with stop words and their count\n",
    "stop_words_df = stop_words_count_df(docList)\n",
    "\n",
    "# Task 4: Generate .txt file excluding stop words\n",
    "remove_stop_words(docList)\n",
    "\n",
    "# Task 5: Print count of lowercase words in the file\n",
    "lowercase_words_count(docList)\n",
    "\n",
    "# Task 6: Perform lemmatization on each document and generate corresponding files\n",
    "perform_lemmatization(docList)\n",
    "\n",
    "# Task 7: Perform stemming on each document and generate corresponding files\n",
    "perform_stemming(docList)\n",
    "\n",
    "# Calculating TF-IDF scores for documents\n",
    "tfidf_list = tfidf(docList)\n",
    "\n",
    "# Calculating IDF scores for words in the vocabulary\n",
    "vocabulary = wordDocFrequency([termFrequencyInDoc(removePuncs(preprocessing(doc))) for doc in docList])\n",
    "idf = inverseDocFrequency(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e72372-41f5-401d-b340-4454ec7cb2f1",
   "metadata": {},
   "source": [
    "Retrieving relevant documents for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12719e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Computer Graphics\n",
      "Rank 1: A00-1007.pdf.txt (Score: 0.1250)\n",
      "Disti l l ing dialogues - A method using natural dialogue \n",
      "dialogue systems development \n",
      "Arne  JSnsson  and  N i l s  Dah lb~ick  \n",
      "Depar tment  of Computer  and  In format ion  Sc ience \n",
      "L inkSp ing  Un ivers i ty  \n",
      "S-581 83, L INKOPING \n",
      "SWEDEN \n",
      "nilda@ida.liu.se, arnjo@ida.liu.se \n",
      "corpora for \n",
      "Abst ract  \n",
      "We report on a method for utilising corpora col- \n",
      "lected in natural settings. It is based on distilling \n",
      "(re-writing) natural dialogues to elicit the type of \n",
      "dialogue that would occur if one t\n",
      "\n",
      "Rank 2: A00-1046.pdf.txt (Score: 0.1171)\n",
      "The Efficiency of Multimodal Interaction for a Map-based Task \n",
      "Philip COHEN, David McGEE, Josh CLOW \n",
      "Center for Human-Computer Communication \n",
      "Oregon Graduate Institute of Science & Technology \n",
      "20000 N.W. Walker Road \n",
      "Beaverton, Oregon 97006 \n",
      "{ pcohen, dmcgee } @cse.ogi.edu \n",
      "Abstract \n",
      "This paper compares the efficiency of using a \n",
      "standard direct-manipulation graphical user \n",
      "interface (GUI) with that of using the QuickSet \n",
      "pen/voice multimodal interface for supporting a \n",
      "military task. In this ta\n",
      "\n",
      "Rank 3: A00-1001.pdf.txt (Score: 0.0642)\n",
      "BusTUC - A natura l  l anguage bus  route  o rac le  \n",
      "Tore Amble \n",
      "Dept. of computer and information science \n",
      "University of Trondheim \n",
      "Norway, N-7491 \n",
      "amble@idi, ntnu. no \n",
      "Abstract \n",
      "The paper describes a natural anguage based expert \n",
      "system route advisor for the public bus transport \n",
      "in Trondheim, Norway. The system is available on \n",
      "the Internet,and has been intstalled at the bus com- \n",
      "pany's web server since the beginning of 1999. The \n",
      "system is bilingual, relying on an internal anguage \n",
      "indepen\n",
      "\n",
      "Rank 4: A00-1017.pdf.txt (Score: 0.0569)\n",
      "A Representation for Complex and Evolving Data Dependencies \n",
      "in Generation \n",
      "C Me l l i sh  $, R Evans  t, L Cah i l l  t, C Doran  t, D Pa iva  t, M Reape $, D Scot t  t, N T ipper  t \n",
      "t Information Technology Research Institute, University of Brighton, Lewes Rd, Brighton, UK \n",
      "SDivision of Informatics, University of Edinburgh, 80 South Bridge, Edinburgh, UK \n",
      "rags@itri, brighton, ac. uk \n",
      "http :/www. itri. brighton, ac. uk/proj ect s/rags \n",
      "Abst rac t  \n",
      "This paper introduces an approach to represen\n",
      "\n",
      "Rank 5: A00-2008.pdf.txt (Score: 0.0567)\n",
      "The FrameNet tagset for frame-semantic and syntactic coding of \n",
      "predicate-argument s ructure \n",
      "Christopher JOHNSON \n",
      "FrameNet Project, \n",
      "International Computer Science Institute \n",
      "currently: Soliloquy, Inc. \n",
      "255 Park Ave S \n",
      "New York NY 10010 \n",
      "c johnson@soliloquy.corn \n",
      "Charles J. FILLMORE \n",
      "FrameNet Project, \n",
      "International Computer Science Institute \n",
      "1947 Center St., Suite 600 \n",
      "Berkeley, CA, USA 94704 \n",
      "fillmore@icsi.berkeley.edu \n",
      "Abstract \n",
      "This paper presents the syntactic and semantic \n",
      "tags used to a\n",
      "\n",
      "--------------------------------------------------\n",
      "Query: natural settings\n",
      "Rank 1: A00-1001.pdf.txt (Score: 0.2819)\n",
      "BusTUC - A natura l  l anguage bus  route  o rac le  \n",
      "Tore Amble \n",
      "Dept. of computer and information science \n",
      "University of Trondheim \n",
      "Norway, N-7491 \n",
      "amble@idi, ntnu. no \n",
      "Abstract \n",
      "The paper describes a natural anguage based expert \n",
      "system route advisor for the public bus transport \n",
      "in Trondheim, Norway. The system is available on \n",
      "the Internet,and has been intstalled at the bus com- \n",
      "pany's web server since the beginning of 1999. The \n",
      "system is bilingual, relying on an internal anguage \n",
      "indepen\n",
      "\n",
      "Rank 2: A00-1007.pdf.txt (Score: 0.1589)\n",
      "Disti l l ing dialogues - A method using natural dialogue \n",
      "dialogue systems development \n",
      "Arne  JSnsson  and  N i l s  Dah lb~ick  \n",
      "Depar tment  of Computer  and  In format ion  Sc ience \n",
      "L inkSp ing  Un ivers i ty  \n",
      "S-581 83, L INKOPING \n",
      "SWEDEN \n",
      "nilda@ida.liu.se, arnjo@ida.liu.se \n",
      "corpora for \n",
      "Abst ract  \n",
      "We report on a method for utilising corpora col- \n",
      "lected in natural settings. It is based on distilling \n",
      "(re-writing) natural dialogues to elicit the type of \n",
      "dialogue that would occur if one t\n",
      "\n",
      "Rank 3: A00-1000.pdf.txt (Score: 0.1442)\n",
      "Association for \n",
      "Computational Linguistics \n",
      "6 th Applied Natural Language Processing \n",
      "Conference \n",
      "Proceedings of the Conference \n",
      "April 29--May 4, 2000 \n",
      "Seattle, Washington, USA \n",
      "ANLP 2000-PREFACE \n",
      "131 papers were submitted to ANLP-2000. 46 were accepted for presentation at the conference. \n",
      "Papers came from 24 countries: fifty eight from the United States of America, eleven each from \n",
      "Germany and United Kingdom, nine from Canada, eight from Japan, four each from Italy and \n",
      "Spain, three ach from F\n",
      "\n",
      "Rank 4: A00-1005.pdf.txt (Score: 0.1177)\n",
      "PartslD: A Dialogue-Based System for Identifying Parts for Medical \n",
      "Systems \n",
      "Amit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISE \n",
      "Information Technology Laboratory \n",
      "GE Corporate Research and Development \n",
      "1 Research Circle \n",
      "Niskayuna, USA, NY 12309 \n",
      "{ bagga, strzalkowski, wisegb } @crd.ge.com \n",
      "Abstract \n",
      "This paper describes a system that \n",
      "provides customer service by allowing \n",
      "users to retrieve identification umbers of \n",
      "parts for medical systems using spoken \n",
      "natural language dialogue. The paper a\n",
      "\n",
      "Rank 5: A00-1046.pdf.txt (Score: 0.0980)\n",
      "The Efficiency of Multimodal Interaction for a Map-based Task \n",
      "Philip COHEN, David McGEE, Josh CLOW \n",
      "Center for Human-Computer Communication \n",
      "Oregon Graduate Institute of Science & Technology \n",
      "20000 N.W. Walker Road \n",
      "Beaverton, Oregon 97006 \n",
      "{ pcohen, dmcgee } @cse.ogi.edu \n",
      "Abstract \n",
      "This paper compares the efficiency of using a \n",
      "standard direct-manipulation graphical user \n",
      "interface (GUI) with that of using the QuickSet \n",
      "pen/voice multimodal interface for supporting a \n",
      "military task. In this ta\n",
      "\n",
      "--------------------------------------------------\n",
      "Query: natural language system\n",
      "Rank 1: A00-1001.pdf.txt (Score: 0.6587)\n",
      "BusTUC - A natura l  l anguage bus  route  o rac le  \n",
      "Tore Amble \n",
      "Dept. of computer and information science \n",
      "University of Trondheim \n",
      "Norway, N-7491 \n",
      "amble@idi, ntnu. no \n",
      "Abstract \n",
      "The paper describes a natural anguage based expert \n",
      "system route advisor for the public bus transport \n",
      "in Trondheim, Norway. The system is available on \n",
      "the Internet,and has been intstalled at the bus com- \n",
      "pany's web server since the beginning of 1999. The \n",
      "system is bilingual, relying on an internal anguage \n",
      "indepen\n",
      "\n",
      "Rank 2: A00-1005.pdf.txt (Score: 0.5525)\n",
      "PartslD: A Dialogue-Based System for Identifying Parts for Medical \n",
      "Systems \n",
      "Amit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISE \n",
      "Information Technology Laboratory \n",
      "GE Corporate Research and Development \n",
      "1 Research Circle \n",
      "Niskayuna, USA, NY 12309 \n",
      "{ bagga, strzalkowski, wisegb } @crd.ge.com \n",
      "Abstract \n",
      "This paper describes a system that \n",
      "provides customer service by allowing \n",
      "users to retrieve identification umbers of \n",
      "parts for medical systems using spoken \n",
      "natural language dialogue. The paper a\n",
      "\n",
      "Rank 3: A00-1002.pdf.txt (Score: 0.5406)\n",
      "Machine Translation of Very Close Languages \n",
      "Jan HAJI(~ \n",
      "Computer Science Dept. \n",
      "Johns Hopkins University \n",
      "3400 N. Charles St., Baltimore, \n",
      "MD 21218, USA \n",
      "hajic@cs.jhu.edu \n",
      "Jan HRIC \n",
      "KTI MFF UK \n",
      "Malostransk6 nfim.25 \n",
      "Praha 1, Czech Republic, 11800 \n",
      "hric@barbora.m ff.cuni.cz \n",
      "Vladislav KUBON \n",
      "OFAL MFF UK \n",
      "Malostransk6 mim.25 \n",
      "Praha 1, Czech Republic, 11800 \n",
      "vk@ufal.mff.cuni.cz \n",
      "Abstract \n",
      "Using examples of the transfer-based MT \n",
      "system between Czech and Russian \n",
      "RUSLAN and the word-for-word MT sys\n",
      "\n",
      "Rank 4: A00-1010.pdf.txt (Score: 0.4617)\n",
      "TALK'N'TRAVEL: A CONVERSATIONAL SYSTEM FOR AIR \n",
      "TRAVEL PLANNING \n",
      "David Stallard \n",
      "BBN Technologies, GTE \n",
      "70 Fawcett St. \n",
      "Cambridge, MA, USA, 02238 \n",
      "Stallard@bbn.com \n",
      "Abstract \n",
      "We describe Talk'n'Travel, a spoken \n",
      "dialogue language system for making air \n",
      "travel plans over the telephone. \n",
      "Talk'n'Travel is a fully conversational, \n",
      "mixed-initiative system that allows the \n",
      "user to specify the constraints on his travel \n",
      "plan in arbitrary order, ask questions, etc., \n",
      "in general spoken English. The syste\n",
      "\n",
      "Rank 5: A00-1007.pdf.txt (Score: 0.4612)\n",
      "Disti l l ing dialogues - A method using natural dialogue \n",
      "dialogue systems development \n",
      "Arne  JSnsson  and  N i l s  Dah lb~ick  \n",
      "Depar tment  of Computer  and  In format ion  Sc ience \n",
      "L inkSp ing  Un ivers i ty  \n",
      "S-581 83, L INKOPING \n",
      "SWEDEN \n",
      "nilda@ida.liu.se, arnjo@ida.liu.se \n",
      "corpora for \n",
      "Abst ract  \n",
      "We report on a method for utilising corpora col- \n",
      "lected in natural settings. It is based on distilling \n",
      "(re-writing) natural dialogues to elicit the type of \n",
      "dialogue that would occur if one t\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    top_5_documents = vectorSpaceModel(query, tfidf_list, docList)[:5]\n",
    "    for rank, (doc_idx, score) in enumerate(top_5_documents):\n",
    "        doc_filename = doc_names[doc_idx]\n",
    "        print(f\"Rank {rank+1}: {doc_filename} (Score: {score:.4f})\")\n",
    "        with open(os.path.join(directory_path, doc_filename), 'r') as file:\n",
    "            print(file.read()[:500])  # Display the first 500 characters of the document\n",
    "        print()\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc3acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10766de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
